2020-03-11 05:16:46,569 WARNING Process rank: -1, device: cuda, n_gpu: 4, distributed training: False, 16-bits training: False
2020-03-11 05:16:52,908 INFO Training/evaluation parameters Namespace(adam_epsilon=1e-08, albert_add=-1, albert_set=-1, bert_dup=-1, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name='bertd6_', model_name_or_path='save/bertd6/bertd6_checkpoint-11000', model_type='bfqa', n_best_size=5, n_gpu=4, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='save/bertd6', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='data/dev-v2.0.json', project_dir='logs/', save_steps=1000, seed=42, server_ip='', server_port='', threads=32, tokenizer_name='', train_file='data/train-v2.0.json', verbose_logging=False, version_2_with_negative=True, warmup_steps=0, weight_decay=0.0)
2020-03-11 05:16:52,909 INFO Creating features from dataset file at .
2020-03-11 05:19:00,839 INFO Saving features into cached file ./cached_train_bertd6_checkpoint-11000_384
2020-03-11 05:22:42,111 INFO NUMBER PARAMS: 116571650
2020-03-11 05:23:03,463 INFO ***** Running training *****
2020-03-11 05:23:03,463 INFO   Num examples = 131944
2020-03-11 05:23:03,463 INFO   Num Epochs = 3
2020-03-11 05:23:03,464 INFO   Instantaneous batch size per GPU = 8
2020-03-11 05:23:03,464 INFO   Total train batch size (w. parallel, distributed & accumulation) = 32
2020-03-11 05:23:03,464 INFO   Gradient Accumulation steps = 1
2020-03-11 05:23:03,464 INFO   Total optimization steps = 12372
2020-03-11 05:23:03,464 INFO   Continuing training from checkpoint, will skip to saved global_step
2020-03-11 05:23:03,464 INFO   Continuing training from epoch 2
2020-03-11 05:23:03,464 INFO   Continuing training from global step 11000
2020-03-11 05:23:03,464 INFO   Will skip the first 2752 steps in the first epoch
2020-03-11 05:43:37,366 INFO Saving model checkpoint to save/bertd6/bertd6_checkpoint-12000
2020-03-11 05:43:38,135 INFO Saving optimizer and scheduler states to save/bertd6/bertd6_checkpoint-12000
2020-03-11 05:51:13,755 INFO Saving model checkpoint to save/bertd6/bertd6_checkpoint-12372
2020-03-11 05:51:14,519 INFO Saving optimizer and scheduler states to save/bertd6/bertd6_checkpoint-12372
2020-03-11 05:51:14,522 INFO  global_step = 12372, average loss = 0.05027312511801623
2020-03-11 05:51:14,522 INFO Saving model checkpoint to save/bertd6
2020-03-11 05:51:17,836 INFO Loading checkpoints saved during training for evaluation
2020-03-11 05:51:17,836 INFO Evaluate the following checkpoints: ['save/bertd6']
2020-03-11 05:51:20,684 INFO Creating features from dataset file at .
2020-03-11 05:51:29,076 INFO Saving features into cached file ./cached_dev_bertd6_checkpoint-11000_384
2020-03-11 05:51:40,286 INFO ***** Running evaluation  *****
2020-03-11 05:51:40,287 INFO   Num examples = 6372
2020-03-11 05:51:40,287 INFO   Batch size = 32
2020-03-11 05:53:11,069 INFO   Evaluation done in total 90.782114 secs (0.014247 sec per example)
